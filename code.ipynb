{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8cde12a",
   "metadata": {},
   "source": [
    "## ðŸ“˜ RAG-Learn: Retrieval-Augmented Chatbot for Lecture Videos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcb7c51",
   "metadata": {},
   "source": [
    "### This notebook demonstrates how to:\n",
    "\n",
    "#### 1. Extract audio from lecture videos.\n",
    "\n",
    "#### 2. Transcribe the audio into text using Whisper.\n",
    "\n",
    "#### 3. Chunk and embed the transcripts.\n",
    "\n",
    "#### 4. Store them in a FAISS vector database.\n",
    "\n",
    "#### 5. Build a chatbot that answers questions using RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ab37d6",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "318610da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"ANONYMIZED_TELEMETRY\"] = \"false\" ## Sets ANONYMIZED_TELEMETRY = false to avoid telemetry pings from LangChain.\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85395a6a",
   "metadata": {},
   "source": [
    "## Audio Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9ebfa4",
   "metadata": {},
   "source": [
    "### Define Paths and Video Extensions\n",
    "Defines directories where videos and audio outputs are stored.\n",
    "Lists supported video file extensions.\n",
    "Uses tqdm.notebook for progress bars in Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c9ba0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from tqdm.notebook import tqdm  # nice progress bars in notebooks\n",
    "\n",
    "# Paths\n",
    "VIDEOS_DIR = \"data/videos\"\n",
    "AUDIO_DIR = \"data/audio\"\n",
    "\n",
    "VIDEO_EXTENSIONS = (\".mp4\", \".mkv\", \".mov\", \".avi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c5a962",
   "metadata": {},
   "source": [
    "### Collect All Video Files\n",
    "Recursively searches VIDEOS_DIR for video files with supported extensions.\n",
    "Stores absolute paths in video_files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5db37aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 video files.\n"
     ]
    }
   ],
   "source": [
    "video_files = []\n",
    "for root, dirs, files in os.walk(VIDEOS_DIR):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(VIDEO_EXTENSIONS):\n",
    "            video_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Found {len(video_files)} video files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a100c6",
   "metadata": {},
   "source": [
    "### Define Audio Extraction Function\n",
    "Defines a helper function using ffmpeg to convert videos â†’ .wav audio files.\n",
    "Standardizes audio format (mono, 16 kHz) for Whisper compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62a683b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fn to extract audio files from  videos\n",
    "def extract_audio(video_path, audio_path):\n",
    "    os.makedirs(os.path.dirname(audio_path), exist_ok=True)\n",
    "    command = [\n",
    "        \"ffmpeg\",\n",
    "        \"-i\", video_path,\n",
    "        \"-vn\",\n",
    "        \"-acodec\", \"pcm_s16le\",\n",
    "        \"-ar\", \"16000\",\n",
    "        \"-ac\", \"1\",\n",
    "        audio_path,\n",
    "        \"-y\"\n",
    "    ]\n",
    "    subprocess.run(command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b0b41b",
   "metadata": {},
   "source": [
    "### Extract Audio from All Videos\n",
    "Loops through all videos, creates mirrored folder structure in AUDIO_DIR, and saves .wav files.\n",
    "\n",
    "Displays progress bar during processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35f6cd2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f40a09acbdd145e084ad8c5d0ba8c0f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting audio:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All audio files extracted!\n"
     ]
    }
   ],
   "source": [
    "# Extract audio with progress bar\n",
    "for video_path in tqdm(video_files, desc=\"Extracting audio\"):\n",
    "    relative_path = os.path.relpath(video_path, VIDEOS_DIR)\n",
    "    audio_path = os.path.join(AUDIO_DIR, os.path.splitext(relative_path)[0] + \".wav\")\n",
    "    extract_audio(video_path, audio_path)\n",
    "\n",
    "print(\"All audio files extracted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6626f493",
   "metadata": {},
   "source": [
    "## Transcribing Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11244972",
   "metadata": {},
   "source": [
    "### Load Whisper Model\n",
    "Loads OpenAIâ€™s Whisper speech-to-text model.\n",
    "\n",
    "Provides directory paths for audio input and transcript output.\n",
    "\n",
    "Model size can be tuned for speed/accuracy tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b514b758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "# Paths\n",
    "AUDIO_DIR = \"data/audio\"\n",
    "TRANSCRIPTS_DIR = \"data/transcripts\"\n",
    "\n",
    "# Load Whisper model (small or medium for speed, large for accuracy)\n",
    "model = whisper.load_model(\"small\")  # or \"medium\", \"large\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb689b61",
   "metadata": {},
   "source": [
    "### Collect Audio Files\n",
    "Finds all .wav audio files for transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fa282c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 audio files.\n"
     ]
    }
   ],
   "source": [
    "# Collect all audio files\n",
    "audio_files = []\n",
    "for root, dirs, files in os.walk(AUDIO_DIR):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(\".wav\"):\n",
    "            audio_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Found {len(audio_files)} audio files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94a2736",
   "metadata": {},
   "source": [
    "### Transcribe Audio Files\n",
    "Iterates through all audio files.\n",
    "\n",
    "Skips transcription if transcript already exists.\n",
    "\n",
    "Saves transcriptions as .txt files in mirrored directory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a19c7ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06577a30756a4d58b44b323327160058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transcribing audio:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All transcripts generated!\n"
     ]
    }
   ],
   "source": [
    "for audio_path in tqdm(audio_files, desc=\"Transcribing audio\"):\n",
    "    # Create mirrored transcript path\n",
    "    relative_path = os.path.relpath(audio_path, AUDIO_DIR)\n",
    "    transcript_path = os.path.join(TRANSCRIPTS_DIR, os.path.splitext(relative_path)[0] + \".txt\")\n",
    "    os.makedirs(os.path.dirname(transcript_path), exist_ok=True)\n",
    "    \n",
    "    # Skip if transcript already exists\n",
    "    if os.path.exists(transcript_path):\n",
    "        continue\n",
    "    \n",
    "    # Transcription\n",
    "    result = model.transcribe(audio_path)\n",
    "    text = result[\"text\"]\n",
    "    \n",
    "    # Save transcript\n",
    "    with open(transcript_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "print(\"All transcripts generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6506458",
   "metadata": {},
   "source": [
    "### Load Transcripts into Documents\n",
    "Loads all transcripts into LangChain Document objects for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7493ebcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 24 documents\n"
     ]
    }
   ],
   "source": [
    "TRANSCRIPTS_DIR = \"data/transcripts\"\n",
    "\n",
    "documents_before_split = []\n",
    "\n",
    "for root, dirs, files in os.walk(TRANSCRIPTS_DIR):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(\".txt\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "            docs = loader.load()\n",
    "            documents_before_split.extend(docs)\n",
    "\n",
    "print(f\"Loaded {len(documents_before_split)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dcde21",
   "metadata": {},
   "source": [
    "### Verify Document Paths\n",
    "Confirms all transcripts are loaded and paths are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "618dad20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data/transcripts/ Hands-On AI: Retrieval Augmented Generation (RAG)/[2] Rag Overview/[1] Architecture of a RAG app/[1] Architecture of a RAG app.txt', 'data/transcripts/ Hands-On AI: Retrieval Augmented Generation (RAG)/[2] Rag Overview/[3] Introduction to embedding models /[3] Introduction to embedding models.txt', 'data/transcripts/ Hands-On AI: Retrieval Augmented Generation (RAG)/[3] Beyond the Basics/[1] Understanding your RAG app with observability /[1] Understanding your RAG app with observability.txt', 'data/transcripts/ Hands-On AI: Retrieval Augmented Generation (RAG)/[3] Beyond the Basics/[2] Begin optimizing your data ingestion /[2] Begin optimizing your data ingestion.txt', 'data/transcripts/ Hands-On AI: Retrieval Augmented Generation (RAG)/[2] Rag Overview/[5] Demo: Calling an LLM /[5] Demo: Calling an LLM.txt', 'data/transcripts/ Hands-On AI: Retrieval Augmented Generation (RAG)/[3] Beyond the Basics/[9] Solution: Different embedding models /[9] Solution: Different embedding models.txt', 'data/transcripts/ Hands-On AI: Retrieval Augmented Generation (RAG)/[2] Rag Overview/[8] Challenge: Putting it all together /[8] Challenge: Putting it all together.txt', 'data/transcripts/ Hands-On AI: Retrieval Augmented Generation (RAG)/[2] Rag Overview/[2] Introduction to LLM usage /[2] Introduction to LLM usage.txt', 'data/transcripts/ Hands-On AI: Retrieval Augmented Generation (RAG)/[3] Beyond the Basics/[7] Solution: Altered data ingestion /[7] Solution: Altered data ingestion.txt', 'data/transcripts/ Hands-On AI: Retrieval Augmented Generation (RAG)/[3] Beyond the Basics/[11] Solution: Comparing results /[11] Solution: Comparing results.txt', 'data/transcripts/ Hands-On AI: Retrieval Augmented Generation (RAG)/[3] Beyond the Basics/[5] Demo: Adding observability to RAG /[5] Demo: Adding observability to RAG.txt', 'data/transcripts/ Hands-On AI: Retrieval Augmented Generation (RAG)/[2] Rag Overview/[4] Introduction to vector databases /[4] Introduction to vector databases.txt', \"data/transcripts/ Hands-On AI: Retrieval Augmented Generation (RAG)/[4] Conclusion/[2] What's next /[2] What's next.txt\", 'data/transcripts/ Hands-On AI: Retrieval Augmented Generation (RAG)/[3] Beyond the Basics/[10] Challenge: Comparing results /[10] Challenge: Comparing results.txt', 'data/transcripts/ Hands-On AI: Retrieval Augmented Generation (RAG)/[2] Rag Overview/[9] Solution: Putting it all together /[9] Solution: Putting it all together.txt', 'data/transcripts/ Hands-On AI: Retrieval Augmented Generation (RAG)/[3] Beyond the Basics/[3] Different embedding models /[3] Different embedding models.txt', 'data/transcripts/ Hands-On AI: Retrieval Augmented Generation (RAG)/[4] Conclusion/[1] Market overview: Available tools /[1] Market overview: Available tools.txt', 'data/transcripts/ Hands-On AI: Retrieval Augmented Generation (RAG)/[3] Beyond the Basics/[6] Challenge: Altered data ingestion /[6] Challenge: Altered data ingestion.txt', 'data/transcripts/ Hands-On AI: Retrieval Augmented Generation (RAG)/[3] Beyond the Basics/[4] Different ways to compare vectors /[4] Different ways to compare vectors.txt', 'data/transcripts/ Hands-On AI: Retrieval Augmented Generation (RAG)/[3] Beyond the Basics/[8] Challenge: Different embedding models /[8] Challenge: Different embedding models.txt', 'data/transcripts/ Hands-On AI: Retrieval Augmented Generation (RAG)/[1] Introduction/[1] Hands-on RAG: Build powerful AI applications /[1] Hands-on RAG: Build powerful AI applications.txt', 'data/transcripts/ Hands-On AI: Retrieval Augmented Generation (RAG)/[1] Introduction/[2] Using GitHub Codespaces and models /[2] Using GitHub Codespaces and models.txt', 'data/transcripts/ Hands-On AI: Retrieval Augmented Generation (RAG)/[2] Rag Overview/[7] Demo: Using a vector database /[7] Demo: Using a vector database.txt', 'data/transcripts/ Hands-On AI: Retrieval Augmented Generation (RAG)/[2] Rag Overview/[6] Demo: Generating an embedding /[6] Demo: Generating an embedding.txt'}\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "# Check original document paths\n",
    "paths = [doc.metadata['source'] for doc in documents_before_split]\n",
    "print(set(paths))   # unique file paths\n",
    "print(len(paths))   # total files loaded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e0deb4",
   "metadata": {},
   "source": [
    "### Check Sample Document Length\n",
    "Prints length of first transcript to understand text size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb2381f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2192"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents_before_split[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8a478c",
   "metadata": {},
   "source": [
    "### Split Documents into Chunks\n",
    "Splits long transcripts into manageable overlapping chunks for embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44816a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 700, ## max size of every chunk\n",
    "    chunk_overlap = 50, ## common elements between the current chunk and the one after to preserve correlation\n",
    ")\n",
    "\n",
    "documents_after_split = text_splitter.split_documents(documents_before_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af6a4d9",
   "metadata": {},
   "source": [
    "### Check Chunk Size\n",
    "Confirms chunking worked (new length ~700)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c03eafaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "697"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents_after_split[0].page_content)\n",
    "## old length = 2192, new length = 697\n",
    "## still within the 700 size range\n",
    "## the next chunk will include the last 100 of the current chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eab35dd",
   "metadata": {},
   "source": [
    "### Compare Average Lengths Before/After Splitting\n",
    "Computes average document length before vs. after splitting.\n",
    "\n",
    "Ensures chunking reduced sizes as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2acfe894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before splitting: 1227\n",
      "After splitting: 551\n"
     ]
    }
   ],
   "source": [
    "avg_doc_length = lambda docs: sum([len(doc.page_content) for doc in docs])//len(docs)\n",
    "\n",
    "avg_char_before_split = avg_doc_length(documents_before_split)\n",
    "avg_char_after_split = avg_doc_length(documents_after_split)\n",
    "\n",
    "print(f'Before splitting: {avg_char_before_split}')\n",
    "print(f'After splitting: {avg_char_after_split}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1192b7b",
   "metadata": {},
   "source": [
    "### Define Embedding Function\n",
    "Loads HuggingFace sentence transformer to embed text chunks into dense vectors.\n",
    "\n",
    "Runs on GPU (cuda) if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "907a4cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-14 22:48:12.243284: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-14 22:48:12.301932: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-14 22:48:13.833647: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "## Embedding function\n",
    "huggingface_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2', ## famous embedding model\n",
    "    model_kwargs = {'device': 'cuda'},\n",
    "    encode_kwargs = {'normalize_embeddings' : True}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf342da7",
   "metadata": {},
   "source": [
    "### Build FAISS Vector Store\n",
    "Creates a FAISS index from all document embeddings.\n",
    "\n",
    "Allows fast similarity search for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24110e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vector database\n",
    "vector_store = FAISS.from_documents(documents = documents_after_split, embedding = huggingface_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41be661d",
   "metadata": {},
   "source": [
    "### Test Similarity Search\n",
    "Runs a test query against the FAISS store.\n",
    "\n",
    "Displays one retrieved document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "841e8ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's start with the definition. What is RAG? RAG stands for Retrieval Augmented Generation. The basic principle behind this technique is to give context to your LLMs so they can answer questions better. There are four main pieces of a RAG application. First, the language model. Usually, this refers to a large language model, but people today are now developing smaller language models that may be able to do the same job. Second, the embedding model. This is what transforms your data into vectors. Third, the vector database. This is where you store your vectorized data. Optionally, a framework to make building your RAG app easier. We'll go into these pieces in detail in later videos. The\n"
     ]
    }
   ],
   "source": [
    "query = 'What is RAG?'\n",
    "relevant_docs = vector_store.similarity_search(query, k=3)\n",
    "print(relevant_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f926c882",
   "metadata": {},
   "source": [
    "### Create Retriever\n",
    "Converts FAISS store into a retriever object for RAG pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dca06ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type = 'similarity', search_kwargs = {'k' : 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae520b3",
   "metadata": {},
   "source": [
    "### Load HuggingFace LLM\n",
    "Loads FLAN-T5 (a lightweight LLM) for answer generation.\n",
    "\n",
    "Configured for controlled, concise outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb8d04f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "        model_id=\"google/flan-t5-base\",\n",
    "        task=\"text2text-generation\",\n",
    "        model_kwargs={\n",
    "            \"temperature\": 0.1,\n",
    "            \"max_length\": 128,\n",
    "            \"do_sample\": True\n",
    "        },\n",
    "        pipeline_kwargs={\n",
    "            \"max_new_tokens\": 128\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef820313",
   "metadata": {},
   "source": [
    "### Define Custom Prompt Template\n",
    "Provides instructions to the LLM on how to structure answers.\n",
    "\n",
    "Limits length and enforces honesty about unknowns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d835202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom prompt template\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. Please follow the following rules:\n",
    "1. If you don't know the answer, just say \"I don't know\".\n",
    "2. If you find the answer, write the answer in a concise way with five sentences maximum.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "      template=prompt_template,\n",
    "      input_variables=[\"context\", \"question\"]\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea60747",
   "metadata": {},
   "source": [
    "### Build Retrieval-Augmented QA Chain\n",
    "Creates a RetrievalQA chain that:\n",
    "\n",
    "    1. Retrieves relevant transcript chunks.\n",
    "\n",
    "    2. Passes them + question to the LLM.\n",
    "\n",
    "    3. Returns generated answer + sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "faefc1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "retrievalQA = RetrievalQA.from_chain_type(\n",
    "    llm = hf,\n",
    "    chain_type = 'stuff',\n",
    "    retriever = retriever,\n",
    "    return_source_documents = True,\n",
    "    chain_type_kwargs = {'prompt' : PROMPT}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b663deb7",
   "metadata": {},
   "source": [
    "### Run Query Through RAG System\n",
    "Runs the test query \"What is RAG?\" through the QA pipeline.\n",
    "\n",
    "Returns structured result with answer + sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a4a9ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What is RAG?', 'result': 'RAG stands for Retrieval Augmented Generation. The basic principle behind this technique is to give context to your LLMs so they can answer questions better. There are four main pieces of a RAG application. First, the language model. Usually, this refers to a large language model, but people today are now developing smaller language models that may be able to do the same job. Second, the embedding model. This is what transforms your data into vectors. Third, the vector database. This is where you store your vectorized data. Optionally, a framework to make', 'source_documents': [Document(metadata={'source': 'data/transcripts/ Hands-On AI: Retrieval Augmented Generation (RAG)/[2] Rag Overview/[1] Architecture of a RAG app/[1] Architecture of a RAG app.txt'}, page_content=\"Let's start with the definition. What is RAG? RAG stands for Retrieval Augmented Generation. The basic principle behind this technique is to give context to your LLMs so they can answer questions better. There are four main pieces of a RAG application. First, the language model. Usually, this refers to a large language model, but people today are now developing smaller language models that may be able to do the same job. Second, the embedding model. This is what transforms your data into vectors. Third, the vector database. This is where you store your vectorized data. Optionally, a framework to make building your RAG app easier. We'll go into these pieces in detail in later videos. The\"), Document(metadata={'source': 'data/transcripts/ Hands-On AI: Retrieval Augmented Generation (RAG)/[3] Beyond the Basics/[11] Solution: Comparing results /[11] Solution: Comparing results.txt'}, page_content=\"Arts Conference in 2013. The answers are semantically the same, but they're structured a little bit differently. There may be other differences that you might get as you play around with the different setups for your RAG application, but that's what we're seeing this time. I hope you enjoyed that challenge.\"), Document(metadata={'source': 'data/transcripts/ Hands-On AI: Retrieval Augmented Generation (RAG)/[3] Beyond the Basics/[4] Different ways to compare vectors /[4] Different ways to compare vectors.txt'}, page_content=\"One last concept to understand for a more complete understanding of RAG is how vectors are compared. There are three main ways to compare vector embeddings. Cosine distance, inner product, and Euclidean distance. Cosine distance measures the difference of the angles between vectors in hyperspace. This is typically the most expensive way to measure distance due to it being higher compute. It's quite popular due to a historical reason. Early NLP papers often use cosine distance on normalized vectors. Inner product is sometimes also referred to as dot product. It measures the projection of one vector onto another. It is the cheapest or least compute expensive of the three mentioned methods.\")]}\n"
     ]
    }
   ],
   "source": [
    "result = retrievalQA.invoke({'query' : query})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae62d4bc",
   "metadata": {},
   "source": [
    "### Inspect Results\n",
    "Prints available result fields.\n",
    "\n",
    "Displays retrieved documents and their original transcript sources for transparency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "776c6094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['query', 'result', 'source_documents'])\n",
      "There are 3 documents retrieved which are relevant to the query.\n",
      "****************************************************************************************************\n",
      "Relevant Document #1:\n",
      "Source file: data/transcripts/ Hands-On AI: Retrieval Augmented Generation (RAG)/[2] Rag Overview/[1] Architecture of a RAG app/[1] Architecture of a RAG app.txt\n",
      "Content:\n",
      "Let's start with the definition. What is RAG? RAG stands for Retrieval Augmented Generation. The basic principle behind this technique is to give context to your LLMs so they can answer questions better. There are four main pieces of a RAG application. First, the language model. Usually, this refers to a large language model, but people today are now developing smaller language models that may be able to do the same job. Second, the embedding model. This is what transforms your data into vectors. Third, the vector database. This is where you store your vectorized data. Optionally, a framework to make building your RAG app easier. We'll go into these pieces in detail in later videos. The\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Relevant Document #2:\n",
      "Source file: data/transcripts/ Hands-On AI: Retrieval Augmented Generation (RAG)/[3] Beyond the Basics/[11] Solution: Comparing results /[11] Solution: Comparing results.txt\n",
      "Content:\n",
      "Arts Conference in 2013. The answers are semantically the same, but they're structured a little bit differently. There may be other differences that you might get as you play around with the different setups for your RAG application, but that's what we're seeing this time. I hope you enjoyed that challenge.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Relevant Document #3:\n",
      "Source file: data/transcripts/ Hands-On AI: Retrieval Augmented Generation (RAG)/[3] Beyond the Basics/[4] Different ways to compare vectors /[4] Different ways to compare vectors.txt\n",
      "Content:\n",
      "One last concept to understand for a more complete understanding of RAG is how vectors are compared. There are three main ways to compare vector embeddings. Cosine distance, inner product, and Euclidean distance. Cosine distance measures the difference of the angles between vectors in hyperspace. This is typically the most expensive way to measure distance due to it being higher compute. It's quite popular due to a historical reason. Early NLP papers often use cosine distance on normalized vectors. Inner product is sometimes also referred to as dot product. It measures the projection of one vector onto another. It is the cheapest or least compute expensive of the three mentioned methods.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(result.keys())\n",
    "\n",
    "relevant_docs = result['source_documents']\n",
    "print(f'There are {len(relevant_docs)} documents retrieved which are relevant to the query.')\n",
    "print(\"*\" * 100)\n",
    "\n",
    "for i, doc in enumerate(relevant_docs):\n",
    "    print(f\"Relevant Document #{i+1}:\")\n",
    "    print(f\"Source file: {doc.metadata['source']}\")\n",
    "    print(f\"Content:\\n{doc.page_content}\")\n",
    "    print(\"-\" * 100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
